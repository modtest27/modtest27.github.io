(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[3364],{77426:(e,t,a)=>{(window.__NEXT_P=window.__NEXT_P||[]).push(["/learn-pages-router/seo/crawling-and-indexing/robots-txt",function(){return a(39581)}])},39581:(e,t,a)=>{"use strict";a.r(t),a.d(t,{default:()=>h,meta:()=>i}),a(10628);var o=a(16942),s=a(18590),n=a(76988),r=a(75597),l=a(79379),c=a(21565),p=a(92797),d=a(55272);let i={title:"Crawling and Indexing",courseId:"seo",lessonId:"crawling-and-indexing",stepId:"robots-txt",subtitle:"What is a robots.txt File?"},m={meta:i},k=e=>{let{children:t}=e;return(0,o.kt)(s.Z,{meta:i},t)};function h(e){let{components:t,...a}=e;return(0,o.kt)(k,{...m,...a,components:t,mdxType:"MDXLayout"},(0,o.kt)(p.P,{fontSize:"16px",mdxType:"P"},"A"," ",(0,o.kt)(d.mj,{href:"https://developers.google.com/search/docs/advanced/robots/intro",mdxType:"Link"},"robots.txt file")," ","tells search engine crawlers which pages or files the crawler can or can't request from your site. The ",(0,o.kt)(r.Z,{mdxType:"InlineCode"},"robots.txt")," file is a web standard file that most"," ",(0,o.kt)(d.mj,{href:"https://www.cloudflare.com/learning/bots/how-to-manage-good-bots",mdxType:"Link"},"good bots")," ","consume before requesting anything from a specific domain."),(0,o.kt)(p.P,{fontSize:"16px",mdxType:"P"},"You might want to protect certain areas from your website from being crawled, and therefore indexed, such as your CMS or admin, user accounts in your e-commerce, or some API routes, to name a few."),(0,o.kt)(p.P,{fontSize:"16px",mdxType:"P"},"These files must be served at the root of each host, or alternatively you can redirect the root ",(0,o.kt)(r.Z,{mdxType:"InlineCode"},"/robots.txt")," path to a destination URL and most bots will follow."),(0,o.kt)("h3",null,"How to add a robots.txt file to a Next.js project"),(0,o.kt)(p.P,{fontSize:"16px",mdxType:"P"},"Thanks to"," ",(0,o.kt)(d.mj,{href:"/docs/basic-features/static-file-serving",mdxType:"Link"},"static file serving")," ","in Next.js we can easily add a `robots.txt` file. , we would create a new file named `robots.txt` the ",(0,o.kt)(r.Z,{mdxType:"InlineCode"},"public")," folder in the root directory."),(0,o.kt)(p.P,{fontSize:"16px",mdxType:"P"},"An example of what you could put in this file would be:"),(0,o.kt)("pre",{className:"language-jsx"},(0,o.kt)("code",{parentName:"pre",className:"language-jsx"},(0,o.kt)("span",{parentName:"code",className:"token comment"},"//robots.txt"),`

# `,(0,o.kt)("span",{parentName:"code",className:"token maybe-class-name"},"Block")," all crawlers ",(0,o.kt)("span",{parentName:"code",className:"token keyword control-flow"},"for")," ",(0,o.kt)("span",{parentName:"code",className:"token operator"},"/"),`accounts
`,(0,o.kt)("span",{parentName:"code",className:"token maybe-class-name"},"User"),(0,o.kt)("span",{parentName:"code",className:"token operator"},"-"),"agent",(0,o.kt)("span",{parentName:"code",className:"token operator"},":")," ",(0,o.kt)("span",{parentName:"code",className:"token operator"},"*"),`
`,(0,o.kt)("span",{parentName:"code",className:"token literal-property property"},"Disallow"),(0,o.kt)("span",{parentName:"code",className:"token operator"},":")," ",(0,o.kt)("span",{parentName:"code",className:"token operator"},"/"),`accounts

# `,(0,o.kt)("span",{parentName:"code",className:"token maybe-class-name"},"Allow"),` all crawlers
`,(0,o.kt)("span",{parentName:"code",className:"token maybe-class-name"},"User"),(0,o.kt)("span",{parentName:"code",className:"token operator"},"-"),"agent",(0,o.kt)("span",{parentName:"code",className:"token operator"},":")," ",(0,o.kt)("span",{parentName:"code",className:"token operator"},"*"),`
`,(0,o.kt)("span",{parentName:"code",className:"token literal-property property"},"Allow"),(0,o.kt)("span",{parentName:"code",className:"token operator"},":")," ",(0,o.kt)("span",{parentName:"code",className:"token operator"},"/"),`
`)),(0,o.kt)(p.P,{fontSize:"16px",mdxType:"P"},"When you run your app with ",(0,o.kt)(r.Z,{mdxType:"InlineCode"},"yarn dev"),", it will now be available at"," ",(0,o.kt)(d.mj,{href:"http://localhost:3000/robots.txt",mdxType:"Link"},"http://localhost:3000/robots.txt"),". Note that the ",(0,o.kt)(r.Z,{mdxType:"InlineCode"},"public")," folder name is not part of the URL."),(0,o.kt)(p.P,{fontSize:"16px",mdxType:"P"},"Do not name the public directory anything else. The name cannot be changed and is the only directory used to serve static assets."),(0,o.kt)(n.Z,{meta:i,answers:["To indicate which pages/files crawlers can access and crawl","To provide crawlers a list of URLs to crawl","To add notes about crawling","All of the above"],correctAnswer:"To indicate which pages/files crawlers can access and crawl",mdxType:"AnswerBox"},(0,o.kt)("br",null),(0,o.kt)(c.H3,{mdxType:"H3"},"Quick Review"),(0,o.kt)(p.P,{fontSize:"16px",mdxType:"P"},"What is the purpose of a robots.txt file?")),(0,o.kt)("div",{style:{width:"100%",padding:"30px 20px",borderRadius:"10px",background:"#F3F3F3"}},(0,o.kt)(c.H3,{mdxType:"H3"},"Further Reading"),(0,o.kt)(l.UL,{mdxType:"UL"},(0,o.kt)(l.LI,{mdxType:"LI"},(0,o.kt)("span",null,"Google:  "),(0,o.kt)(d.mj,{href:"https://developers.google.com/search/docs/advanced/robots/create-robots-txt",mdxType:"Link"},"Create and Submit a ",(0,o.kt)(r.Z,{mdxType:"InlineCode"},"robots.txt")," File")))))}h.isMDXComponent=!0}},e=>{var t=t=>e(e.s=t);e.O(0,[27,8746,4371,2878,4730,1067,8590,3276,2888,9774,179],()=>t(77426)),_N_E=e.O()}]);